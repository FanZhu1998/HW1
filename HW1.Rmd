---
title: "STA521 HW1"
author: '[Fan Zhu | netid: fz63]'
date: "Due August 28th, 2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(knitr)
library(GGally)
library(dplyr)
library(tinytex)
library(tidyverse)
options(tinytex.verbose = TRUE)
# set knit options and loaded libraries
```

This exercise involves the Auto data set from ISLR.  Load the data and answer the following questions adding your code in the code chunks. Please submit a pdf version to Sakai.  For full credit, you should push your final Rmd file to your github repo on the STA521-F19 organization site by the deadline  (the version that is submitted on Sakai will be graded)

```{r data, echo=F}
#load the "Auto" data
data(Auto)
```

## Exploratory Data Analysis
1. Create a summary of the data.  How many variables have missing data?

```{r Exercise1}
summary(Auto)
colSums(is.na(Auto))
#None of the variables has missing data.
```


2.  Which of the predictors are quantitative, and which are qualitative?
```{r Exercise2}
help(Auto)
```
Quantitative: mpg, displacement, horsepower, weight, and acceleration  

Qualitative: origin, cylinders, year and name


3. What is the range of each quantitative predictor? You can answer this using the `range()` function.   Create a table with variable name, min, max with one row per variable.   `kable` from the package `knitr` can display tables nicely.

```{r Exercise3}
ranges_Auto <- rbind(range(Auto$mpg),
                       range(Auto$displacement),
                       range(Auto$horsepower),
                       range(Auto$weight),
                       range(Auto$acceleration))
rownames(ranges_Auto) <- c("mpg","displacement", 
                           "horsepower", "weight", "acceleration")
knitr::kable(ranges_Auto, "pipe", caption = "Table1: Ranges of the Quantitative Variables",
             col.names = c("Min","Max"))
```

4. What is the mean and standard deviation of each quantitative predictor? Format nicely in a table as above

```{r Exercise4}
centers_Auto <- rbind(c(mean(Auto$mpg),sd(Auto$mpg)),
                        c(mean(Auto$displacement),sd(Auto$displacement)),
                        c(mean(Auto$horsepower),sd(Auto$horsepower)),
                        c(mean(Auto$weight),sd(Auto$weight)),
                        c(mean(Auto$acceleration),sd(Auto$acceleration)))
rownames(centers_Auto) <- c("mpg", "displacement", 
                           "horsepower", "weight", "acceleration")
knitr::kable(centers_Auto, "pipe", caption = "Table2: Central Tendencies for the Quantitative Variables",
             col.names = c("Mean","SD"))
```


5. Investigate the predictors graphically, using scatterplot matrices  (`ggpairs`) and other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings. Try adding a caption to your figure
```{r Exercise5, echo=TRUE, fig.width=8, message=FALSE, warning=FALSE}
#create pairwise comparisons scatterplot mattrices using the "ggpairs" function
library(ggplot2)
library(GGally)
ggpairs(Auto[,c(1,3,4,5,6)],title = "Pairwise Comparisons of 5 Quantitative Variables", progress = FALSE)

#create a boxplot of MPG for different Auto Origins
boxplot(mpg~origin,
data=Auto,
main="Distribution of MPG for Different Auto Origins",
xlab="Origin",
ylab="MPG",
names=c("American","European","Japanese")
)

#create a boxplot of MPG for different Auto Years
boxplot(mpg~year,
data=Auto,
main="Distribution of MPG for Different Auto Years",
xlab="year (19XX)",
ylab="MPG"
)

```

6. Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables using regression. Do your plots suggest that any of the other variables might be useful in predicting mpg using linear regression? Justify your answer.

Yes. The scatterplot matrices contain scatterplots between each pair of variables, from which we can observe whether there exists a linear relationship between these variables or not. The plots also contain correlations, which we can use to find which independent variable is correlated with the response/dependent variable. Besides, if the correlations between two independent variables are too high, then there is a multicollinearity issue. 


## Simple Linear Regression

7.  Use the `lm()` function to perform a simple linear 
regression with `mpg` as the response and `horsepower` as the
predictor. Use the `summary()` function to print the results.
Comment on the output.
For example:
    (a) Is there a relationship between the predictor and the response?
    (b) How strong is the relationship between the predictor and
the response?
    (c) Is the relationship between the predictor and the response
positive or negative?
    (d)  Provide a brief interpretation of the parameters that would suitable for discussing with a car dealer, who has little statistical background.
    (e) What is the predicted mpg associated with a horsepower of
98? What are the associated 95% confidence and prediction
intervals?   (see `help(predict)`) Provide interpretations of these for the car dealer.

```{r Exercise7}
#run a regression with mpg as the response and horsepower as the predictor.
model1 <- lm(mpg ~ horsepower, data = Auto)
summary(model1)
#compute the 95% confidence interval given horsepower = 98
conf_int <-  predict(model1, data.frame(horsepower = 98), interval = "confidence") 
conf_int
#compute the 95% prediction interval given horsepower = 98
pred_int <-  predict(model1, data.frame(horsepower = 98), interval = "prediction")
pred_int
```
7(a) Yes. There is a relationship between the predictor and the response. The P value for the beta in the regression is smaller than 0.05. Thus, we can reject the null hypothesis that the beta is zero, which means that there is a significant nonzero relationship between teh predictor and the response. 
7(b) The beta is -0.157845, which means that holding everything else constant, every one unit increase in the horsepower will decrease the mpg by 0.157845. And as we have discussed in 7(a), the beta is significant.
7(c) The relationship between the predictor and the response is negative because the beta is negative.
7(d) Holding everything else constant, every one unit increase in the horsepower will decrease the mpg by 0.157845. That is, higher horsepower leads to more fuel consumption (less miles per gallon). There is a trade-off between performance and the car's costs on fuels. 
7(e) The predicted mpg associated with a horsepower of 98 is 24.46708. We can tell the dealer that with a horsepower of 98, the car would run 24.46708 miles per gallon of gasoline.
The lower bound of the 95% confidence interval is 23.97308 and the upper bound of the confidence interval is 24.96108. 
The lower bound of the 95% prediction interval is 14.8094 and the upper bound of the prediction interval is 34.12476.  
A more stats-version of the confidence interval is that: if we take the samples repeatedly and computed the 95% confidence interval for each sample, 95% of the intervals would contain the true population mean. 
In other words, we are 95% confidence that the average mpg of a car, whose horsepower is 98, will be between 23.97308 and 24.96108. we are also 95% confident that a predicted/new car with 98 horsepower will have its mpg between 14.8094 and 34.12476.  



8. Plot the response and the predictor using `ggplot`.  Add to the plot a line showing the least squares regression line. 
```{r Exercise8}
ggplot(Auto, aes(x = horsepower, y = mpg)) + 
  geom_point() + geom_smooth(method = "lm", formula = y ~ x)
```


9. Use the `plot()` function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the model regarding assumptions for using a simple linear regression.  

```{r Exercise9}
par(mfrow = c(2, 2)) 
plot(model1)
```
Interpretations on Q9 plots:
The "Residual vs. Fitted" plot exhibits a non-linear trend between the residuals and the fitted values, which implies a violation of the linearity assumption of the model.
The normal Q-Q plot shows that most of the data fit the diagonal line except some data on the left and right ends. The ideal case of the Q-Q plot is a perfect match to the diagonal line. Our Q-Q plot shows the data is not very far from a normal distribution but there are still some deviations. 
The Scale-Location plot shows an U-shaped curve, which implies that the variance of the residuals does not stay constant along the fitted values. Therefore, the assumption of homoscedasticity (equal variance) is violated. 
The Residuals vs. Leverage plot shows that almost all the data points are within the Cook's distance lines, which indicates that there is no influential case. 
In conclusion, from the diagnostics plots above, we can find that the residuals may not be normally distributed with a mean of zero and a constant variance. 

## Theory


10. Show that the  regression function $E(Y \mid x) = f(x)$ is the optimal 
optimal predictor of $Y$ given $X = x$ using squared error loss:  that is $f(x)$
minimizes $E[(Y - g(x))^2 \mid X =x]$ over all functions $g(x)$ at all points $X=x$.   Hint:  there are at least two ways to do this.   Differentiation (so think about how to justify) - or - add and subtract the proposed optimal predictor and who that it must minimize the function.




11. (adopted from ELS Ex 2.7 ) Suppose that we have a sample of $N$ pairs $x_i, y_i$ drwan iid from the distribution characterized as follows 
$$ x_i \sim h(x), \text{ the design distribution}$$
$$ \epsilon_i \sim g(y), \text{ with mean 0 and variance } \sigma^2 \text{ and are independent of the } x_i $$
$$Y_i = f(x_i) + \epsilon$$
  (a) What is the conditional expectation of $Y$ given that $X = x_o$?  ($E_{Y \mid X}[Y]$)
  (b) What is the conditional variance of $Y$ given that $X = x_o$? ($\text{Var}_{Y \mid X}[Y]$)
  (c) show  that for any estimator $\hat{f}(x)$ that the conditional (given X) (expected)  Mean Squared Error can be decomposed as 
$$E_{Y \mid X}[(Y - \hat{f}(x_o))^2] = \underbrace{ \text{Var}_{Y \mid X}[\hat{f}(x_o)]}_{\textit{Variance of estimator}} +
\underbrace{(f(x) - E_{Y \mid X}[\hat{f}(x_o)])^2}_{\textit{Squared Bias}} + \underbrace{\textsf{Var}(\epsilon)}_{\textit{Irreducible}}
$$
 _Hint:  try the add zero trick of adding and subtracting expected values_
  (d) Explain why even if $N$ goes to infinity the above can never go to zero.
e.g. even if we can learn $f(x)$ perfectly that the error in prediction will not vanish.   
  (e) Decompose the unconditional mean squared error
$$E_{Y, X}(f(x_o) - \hat{f}(x_o))^2$$
into a squared bias and a variance component. (See ELS 2.7(c))
  (f) Establish a relationship between the squared biases and variance in the above Mean squared errors.
